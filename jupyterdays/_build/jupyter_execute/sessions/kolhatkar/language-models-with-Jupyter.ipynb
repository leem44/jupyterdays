{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Teaching Computational Linguistics with Jupyter\n",
    "\n",
    "\n",
    "## Jupyter Day presentation: Language Models\n",
    "\n",
    "Varada Kolhatkar [ʋəɾəda kɔːlɦəʈkər]\n",
    "\n",
    "Assistant Professor of Teaching in Computer Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Slide settings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-13T16:11:37.473447Z",
     "iopub.status.busy": "2020-08-13T16:11:37.472650Z",
     "iopub.status.idle": "2020-08-13T16:11:37.480614Z",
     "shell.execute_reply": "2020-08-13T16:11:37.479620Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from traitlets.config.manager import BaseJSONConfigManager\n",
    "from pathlib import Path\n",
    "path = Path.home() / \".jupyter\" / \"nbconfig\"\n",
    "cm = BaseJSONConfigManager(config_dir=str(path))\n",
    "tmp = cm.update(\n",
    "        \"rise\",\n",
    "        {\n",
    "            \"theme\": \"serif\",\n",
    "            \"transition\": \"fade\",\n",
    "            \"start_slideshow_at\": \"selected\",            \n",
    "            \"width\": \"100%\",\n",
    "            \"height\": \"100%\",\n",
    "            \"header\": \"\",\n",
    "            \"footer\":\"\",\n",
    "            \"scroll\": True,\n",
    "            \"enable_chalkboard\": True,\n",
    "            \"slideNumber\": True,\n",
    "            \"center\": False,\n",
    "            \"controlsLayout\": \"edges\",\n",
    "            \"slideNumber\": True,\n",
    "            \"hash\": True,\n",
    "        }\n",
    "    )\n",
    "\n",
    "## Set Altair default size\n",
    "\n",
    "def theme_vk(*args, **kwargs):\n",
    "    return {'height': 400,\n",
    "            'config': {'style': {'circle': {'size': 400},\n",
    "                                'point': {'size': 30},\n",
    "                                'square': {'size': 400},\n",
    "                                },\n",
    "                       'legend': {'symbolSize': 20, 'titleFontSize': 20, 'labelFontSize': 20}, \n",
    "                       'axis': {'titleFontSize': 20, 'labelFontSize': 20}},\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-13T16:11:37.490258Z",
     "iopub.status.busy": "2020-08-13T16:11:37.489664Z",
     "iopub.status.idle": "2020-08-13T16:11:39.034008Z",
     "shell.execute_reply": "2020-08-13T16:11:39.033604Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/jupyterdays/lib/python3.7/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['cm']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/tbeuzen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## import the libraries \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import IPython\n",
    "%pylab inline\n",
    "# pip install git+git://github.com/mgelbart/plot-classifier.git\n",
    "from plot_classifier import plot_classifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "\n",
    "# pip install ipython-autotime\n",
    "import autotime\n",
    "\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Goal \n",
    "\n",
    "- To give you a high-level overview of language models.  \n",
    "- Demonstrate how I use Jupyter notebooks to teach with different modalities.  \n",
    "    - Text\n",
    "    - Mathematical equations with latex\n",
    "    - Images s\n",
    "    - Code\n",
    "    - Videos \n",
    "    - Interactive websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-13T16:11:39.039350Z",
     "iopub.status.busy": "2020-08-13T16:11:39.038396Z",
     "iopub.status.idle": "2020-08-13T16:11:39.041074Z",
     "shell.execute_reply": "2020-08-13T16:11:39.041423Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".rendered_html table, .rendered_html th, .rendered_html tr, .rendered_html td {\n",
       "     font-size: 130%;\n",
       "}\n",
       "\n",
       "body.rise-enabled div.inner_cell>div.input_area {\n",
       "    font-size: 100%;\n",
       "}\n",
       "\n",
       "body.rise-enabled div.output_subarea.output_text.output_result {\n",
       "    font-size: 100%;\n",
       "}\n",
       "body.rise-enabled div.output_subarea.output_text.output_stream.output_stdout {\n",
       "  font-size: 150%;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style>\n",
    ".rendered_html table, .rendered_html th, .rendered_html tr, .rendered_html td {\n",
    "     font-size: 130%;\n",
    "}\n",
    "\n",
    "body.rise-enabled div.inner_cell>div.input_area {\n",
    "    font-size: 100%;\n",
    "}\n",
    "\n",
    "body.rise-enabled div.output_subarea.output_text.output_result {\n",
    "    font-size: 100%;\n",
    "}\n",
    "body.rise-enabled div.output_subarea.output_text.output_stream.output_stdout {\n",
    "  font-size: 150%;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-13T16:11:39.044770Z",
     "iopub.status.busy": "2020-08-13T16:11:39.044342Z",
     "iopub.status.idle": "2020-08-13T16:11:39.045989Z",
     "shell.execute_reply": "2020-08-13T16:11:39.046315Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def display_url(url): \n",
    "    \"\"\"\n",
    "    Given a url, display it as an iframe. \n",
    "    \n",
    "    Arguments: \n",
    "    ----------\n",
    "    url : str\n",
    "        The url to be displayed \n",
    "    \n",
    "    Return:\n",
    "    ----------\n",
    "    None\n",
    "    \"\"\"\n",
    "    display(HTML(\"<iframe src=%s width=1000 height=900 allowfullscreen></iframe>\"%url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Which of the following do you use on everyday basis?\n",
    "\n",
    "- Hover over the area above screen sharing to see the toolbar/options and click on \"Annotate\".  \n",
    "- Select \"Stamp\" and put your favourite stamps in the appropriate box(s). \n",
    "\n",
    "<center>\n",
    "<img src=\"images/annotation-exercise.png\" height=\"1400\" width=\"1400\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A common component in all these services is **a language model**!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a language model? \n",
    "\n",
    "A model that computes the probability of a sequence of words or the probability of an upcoming word is called a **language model**.\n",
    "\n",
    "- Compute the probability of a sentence or a sequence of words.\n",
    "    - $P(w_1, w_2,\\dots,w_t)$\n",
    "    - P(I have read this book) > P(eye have red this book)\n",
    "\n",
    "- A related task: What's the probability of an upcoming word? \n",
    "    - $P(w_t|w_1,w_2,\\dots,w_{t-1})$ \n",
    "    - P(book | read this) > P(book | red this)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Language model examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-13T16:11:39.049019Z",
     "iopub.status.busy": "2020-08-13T16:11:39.048601Z",
     "iopub.status.idle": "2020-08-13T16:11:39.052686Z",
     "shell.execute_reply": "2020-08-13T16:11:39.053105Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/jupyterdays/lib/python3.7/site-packages/IPython/core/display.py:717: UserWarning: Consider using IPython.display.IFrame instead\n",
      "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe src=https://ai.googleblog.com/2018/05/smart-compose-using-neural-networks-to.html width=1000 height=900 allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gmail smart compose\n",
    "url = \"https://ai.googleblog.com/2018/05/smart-compose-using-neural-networks-to.html\"\n",
    "display_url(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Voice assistant example\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"images/voice-assistant-ex.png\" height=\"1400\" width=\"1400\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-13T16:11:39.056190Z",
     "iopub.status.busy": "2020-08-13T16:11:39.055640Z",
     "iopub.status.idle": "2020-08-13T16:11:39.057982Z",
     "shell.execute_reply": "2020-08-13T16:11:39.058389Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=https://www.youtube.com/embed/fZSFNUT6iY8?rel=0&amp;controls=0&amp;showinfo=0 width=1000 height=900 allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### An example of a state-of-the-art language model\n",
    "url = \"https://www.youtube.com/embed/fZSFNUT6iY8?rel=0&amp;controls=0&amp;showinfo=0\"\n",
    "display_url(url) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Language modeling: Why should we care?\n",
    "\n",
    "Powerful idea in NLP and helps in many tasks.\n",
    "- Machine translation \n",
    "    * P(In the age of data algorithms have the answer) > P(the age data of in algorithms answer the have)\n",
    "- Spelling correction\n",
    "    * My office is a 20  <span style=\"color:red\">minuet</span> bike ride from my home.  \n",
    "        * P(20 <span style=\"color:blue\">minute</span> bike ride from my home) > P(20 <span style=\"color:red\">minuet</span> bike ride from my home)\n",
    "- Speech recognition \n",
    "    * P(<span style=\"color:blue\">I read</span> a book) > P(<span style=\"color:red\">Eye red</span> a book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A naive way to calculate probabilities of a sentence\n",
    "\n",
    "- Calculate probabilities of a sequence by applying chain rule \n",
    "- Example: Suppose we want to calculate the probability of the following sequence of words: \n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "P(\\textrm{In the age of data algorithms have the answer}) =& P(\\textrm{In}) \\times P(\\textrm{the|In})\\\\ \n",
    "                                              & \\times P(\\textrm{age|In the}) \\times P(\\textrm{of|In the age})\\\\\n",
    "                                              & \\times P(\\textrm{data|In the age of})\\\\\n",
    "                                              & \\times P(\\textrm{algorithms|In the age of data}) \\\\\n",
    "                                              &  \\times P(\\textrm{have|In the age of data algorithms}) \\\\\n",
    "                                              & \\dots \n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "- How often the exact same long sequences of words occur in text? For example, how often the sequence \"In the age of data algorithms have\" is likely to occur in your data? \n",
    "- The counts will be tiny and the model will be very sparse. \n",
    "- <span style=\"color:red\">BAD IDEA!!</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Markov models of language\n",
    "\n",
    "**Markov assumption: The future is conditionally independent of the past given present**\n",
    "<center>\n",
    "<img src=\"images/Markov-assumption.png\" height=\"500\" width=\"500\">\n",
    "</center>\n",
    "\n",
    "- Bigram language model\n",
    "    \n",
    "$$\n",
    "P(\\textrm{algorithms|In the age of data}) \\approx P(\\textrm{algorithms|data})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Markov model of language (bigram language model)\n",
    "\n",
    "- Use Markov assumption and calculate the probability of a sequence as follows!\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "P(\\textrm{In the age of data algorithms have the answer}) =& P(\\textrm{In}) \\times P(\\textrm{the|In})\\\\ \n",
    "                                              & \\times P(\\textrm{age|the})\\\\\n",
    "                                              & \\times P(\\textrm{of|age})\\\\\n",
    "                                              & \\times P(\\textrm{data|of})\\\\\n",
    "                                              & \\times P(\\textrm{algorithms|data}) \\\\                 \n",
    "                                              & \\times P(\\textrm{have|algorithms}) \\\\                             \n",
    "                                              & \\times P(\\textrm{the|have}) \\\\                                   \n",
    "                                              & \\times P(\\textrm{answer|the}) \\\\                                                                                 \n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "### Estimating probabilities for the bigram language model\n",
    "\n",
    "- Example\n",
    "$$P(\\textrm{algorithms|data}) = \\frac{Count(\\textrm{data algorithms})}{Count(\\textrm{data})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Text generation using Markov models of languaage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-13T16:11:39.062038Z",
     "iopub.status.busy": "2020-08-13T16:11:39.061527Z",
     "iopub.status.idle": "2020-08-13T16:11:39.063124Z",
     "shell.execute_reply": "2020-08-13T16:11:39.063472Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "toy_corpus = '''The birds they sang\n",
    "At the break of day\n",
    "Start again\n",
    "I heard them say\n",
    "Don't dwell on what\n",
    "Has passed away\n",
    "Or what is yet to be\n",
    "Yeah the wars they will\n",
    "Be fought again\n",
    "The holy dove\n",
    "She will be caught again\n",
    "Bought and sold\n",
    "And bought again\n",
    "The dove is never free\n",
    "Ring the bells (ring the bells) that still can ring\n",
    "Forget your perfect offering\n",
    "There is a crack in everything (there is a crack in everything)\n",
    "That's how the light gets in\n",
    "We asked for signs\n",
    "The signs were sent\n",
    "The birth betrayed\n",
    "The marriage spent\n",
    "Yeah the widowhood\n",
    "Of every government\n",
    "Signs for all to see\n",
    "I can't run no more\n",
    "With that lawless crowd\n",
    "While the killers in high places\n",
    "Say their prayers out loud\n",
    "But they've summoned, they've summoned up\n",
    "A thundercloud\n",
    "And they're going to hear from me\n",
    "Ring the bells that still can ring\n",
    "Forget your perfect offering\n",
    "There is a crack, a crack in everything (there is a crack in everything)\n",
    "That's how the light gets in\n",
    "You can add up the parts\n",
    "You won't have the sum\n",
    "You can strike up the march\n",
    "There is no drum\n",
    "Every heart, every heart to love will come\n",
    "But like a refugee\n",
    "Ring the bells that still can ring\n",
    "Forget your perfect offering\n",
    "There is a crack, a crack in everything (there is a crack in everything)\n",
    "That's how the light gets in\n",
    "Ring the bells that still can ring (ring the bells that still can ring)\n",
    "Forget your perfect offering\n",
    "There is a crack, a crack in everything (there is a crack in everything)\n",
    "That's how the light gets in\n",
    "That's how the light gets in\n",
    "That's how the light gets in'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-13T16:11:39.067547Z",
     "iopub.status.busy": "2020-08-13T16:11:39.067147Z",
     "iopub.status.idle": "2020-08-13T16:11:39.112114Z",
     "shell.execute_reply": "2020-08-13T16:11:39.111733Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>birds</th>\n",
       "      <th>break</th>\n",
       "      <th>wars</th>\n",
       "      <th>holy</th>\n",
       "      <th>dove</th>\n",
       "      <th>bells</th>\n",
       "      <th>light</th>\n",
       "      <th>signs</th>\n",
       "      <th>birth</th>\n",
       "      <th>marriage</th>\n",
       "      <th>...</th>\n",
       "      <th>out</th>\n",
       "      <th>loud</th>\n",
       "      <th>but</th>\n",
       "      <th>like</th>\n",
       "      <th>summoned</th>\n",
       "      <th>up</th>\n",
       "      <th>going</th>\n",
       "      <th>from</th>\n",
       "      <th>me</th>\n",
       "      <th>wo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>birds</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>they</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sang</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>at</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heart</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>refugee</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115 rows × 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         birds  break  wars  holy  dove  bells  light  signs  birth  marriage  \\\n",
       "the        1.0    1.0   1.0   1.0   1.0    6.0    6.0    1.0    1.0       1.0   \n",
       "birds      0.0    0.0   0.0   0.0   0.0    0.0    0.0    0.0    0.0       0.0   \n",
       "they       0.0    0.0   0.0   0.0   0.0    0.0    0.0    0.0    0.0       0.0   \n",
       "sang       0.0    0.0   0.0   0.0   0.0    0.0    0.0    0.0    0.0       0.0   \n",
       "at         0.0    0.0   0.0   0.0   0.0    0.0    0.0    0.0    0.0       0.0   \n",
       "...        ...    ...   ...   ...   ...    ...    ...    ...    ...       ...   \n",
       "heart      0.0    0.0   0.0   0.0   0.0    0.0    0.0    0.0    0.0       0.0   \n",
       "love       0.0    0.0   0.0   0.0   0.0    0.0    0.0    0.0    0.0       0.0   \n",
       "come       0.0    0.0   0.0   0.0   0.0    0.0    0.0    0.0    0.0       0.0   \n",
       "like       0.0    0.0   0.0   0.0   0.0    0.0    0.0    0.0    0.0       0.0   \n",
       "refugee    0.0    0.0   0.0   0.0   0.0    0.0    0.0    0.0    0.0       0.0   \n",
       "\n",
       "         ...  out  loud  but  like  summoned   up  going  from   me   wo  \n",
       "the      ...  0.0   0.0  0.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "birds    ...  0.0   0.0  0.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "they     ...  0.0   0.0  0.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "sang     ...  0.0   0.0  0.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "at       ...  0.0   0.0  0.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "...      ...  ...   ...  ...   ...       ...  ...    ...   ...  ...  ...  \n",
       "heart    ...  0.0   0.0  0.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "love     ...  0.0   0.0  0.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "come     ...  0.0   0.0  1.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "like     ...  0.0   0.0  0.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "refugee  ...  0.0   0.0  0.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "\n",
       "[115 rows x 115 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_corpus_tokens = nltk.word_tokenize(toy_corpus.lower())\n",
    "\n",
    "frequencies = defaultdict(Counter)\n",
    "for i in range(len(toy_corpus_tokens) - 1):\n",
    "    frequencies[toy_corpus_tokens[i: i + 1][0]][toy_corpus_tokens[i + 1]] += 1\n",
    "    \n",
    "freq_df = pd.DataFrame(frequencies).transpose()\n",
    "freq_df = freq_df.fillna(0)\n",
    "freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-13T16:11:39.116874Z",
     "iopub.status.busy": "2020-08-13T16:11:39.115305Z",
     "iopub.status.idle": "2020-08-13T16:11:39.140125Z",
     "shell.execute_reply": "2020-08-13T16:11:39.139681Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>birds</th>\n",
       "      <th>break</th>\n",
       "      <th>wars</th>\n",
       "      <th>holy</th>\n",
       "      <th>dove</th>\n",
       "      <th>bells</th>\n",
       "      <th>light</th>\n",
       "      <th>signs</th>\n",
       "      <th>birth</th>\n",
       "      <th>marriage</th>\n",
       "      <th>...</th>\n",
       "      <th>out</th>\n",
       "      <th>loud</th>\n",
       "      <th>but</th>\n",
       "      <th>like</th>\n",
       "      <th>summoned</th>\n",
       "      <th>up</th>\n",
       "      <th>going</th>\n",
       "      <th>from</th>\n",
       "      <th>me</th>\n",
       "      <th>wo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>birds</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>they</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sang</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>at</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heart</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>refugee</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115 rows × 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         birds  break  wars  holy  dove  bells  light  signs  birth  marriage  \\\n",
       "the       0.04   0.04  0.04  0.04  0.04   0.24   0.24   0.04   0.04      0.04   \n",
       "birds     0.00   0.00  0.00  0.00  0.00   0.00   0.00   0.00   0.00      0.00   \n",
       "they      0.00   0.00  0.00  0.00  0.00   0.00   0.00   0.00   0.00      0.00   \n",
       "sang      0.00   0.00  0.00  0.00  0.00   0.00   0.00   0.00   0.00      0.00   \n",
       "at        0.00   0.00  0.00  0.00  0.00   0.00   0.00   0.00   0.00      0.00   \n",
       "...        ...    ...   ...   ...   ...    ...    ...    ...    ...       ...   \n",
       "heart     0.00   0.00  0.00  0.00  0.00   0.00   0.00   0.00   0.00      0.00   \n",
       "love      0.00   0.00  0.00  0.00  0.00   0.00   0.00   0.00   0.00      0.00   \n",
       "come      0.00   0.00  0.00  0.00  0.00   0.00   0.00   0.00   0.00      0.00   \n",
       "like      0.00   0.00  0.00  0.00  0.00   0.00   0.00   0.00   0.00      0.00   \n",
       "refugee   0.00   0.00  0.00  0.00  0.00   0.00   0.00   0.00   0.00      0.00   \n",
       "\n",
       "         ...  out  loud  but  like  summoned   up  going  from   me   wo  \n",
       "the      ...  0.0   0.0  0.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "birds    ...  0.0   0.0  0.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "they     ...  0.0   0.0  0.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "sang     ...  0.0   0.0  0.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "at       ...  0.0   0.0  0.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "...      ...  ...   ...  ...   ...       ...  ...    ...   ...  ...  ...  \n",
       "heart    ...  0.0   0.0  0.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "love     ...  0.0   0.0  0.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "come     ...  0.0   0.0  1.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "like     ...  0.0   0.0  0.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "refugee  ...  0.0   0.0  0.0   0.0       0.0  0.0    0.0   0.0  0.0  0.0  \n",
       "\n",
       "[115 rows x 115 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_df = freq_df.div(freq_df.sum(axis=1), axis=0)\n",
    "trans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-13T16:11:39.146448Z",
     "iopub.status.busy": "2020-08-13T16:11:39.146035Z",
     "iopub.status.idle": "2020-08-13T16:11:39.168177Z",
     "shell.execute_reply": "2020-08-13T16:11:39.167736Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE GENERATED SEQUENCE:\n",
      "   the dove is a crack in everything ) that 's how the dove is yet to hear from me ring the killers in that 's how the marriage spent yeah the sum you can strike up the light gets in you wo n't have the birth betrayed the light gets in everything ( ring the bells that 's how the dove she will come but like a crack in everything ) that still can strike up the signs were sent the light gets in everything ) forget your perfect offering there is a refugee ring the killers in high places\n"
     ]
    }
   ],
   "source": [
    "### Generate text using the Markov model above\n",
    "start_char = 'the'\n",
    "seq_len = 100\n",
    "seq = ''\n",
    "ch = start_char\n",
    "for i in range(seq_len):    \n",
    "    seq += \" \" + ch\n",
    "    next_char = npr.choice(trans_df.columns.tolist(), p = trans_df.loc[ch,].values.flatten())\n",
    "    #print('The sampled next character is: ', next_char)\n",
    "    ch = next_char\n",
    "print('THE GENERATED SEQUENCE:\\n ', seq)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### In practice the corpus (dataset) is huge. For example, the full Wikipedia or the text available on the entire Internet, or all the New York Times articles from the last 20 years. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Considering more history \n",
    "\n",
    "- Example: trigrams or four-gram language model\n",
    "    - Trigram language model\n",
    "$$\n",
    "P(\\textrm{algorithms|In the age of data}) \\approx P(\\textrm{algorithms|of data})\n",
    "$$\n",
    "    - Four-gram language model\n",
    "$$\n",
    "P(\\textrm{algorithms|In the age of data}) \\approx P(\\textrm{algorithms|age of data})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [Google n-gram viewer](https://books.google.com/ngrams)\n",
    " \n",
    "- All Our N-gram are Belong to You\n",
    "    - https://ai.googleblog.com/2006/08/all-our-n-gram-are-belong-toyou.html\n",
    "\n",
    "<blockquote>\n",
    "Here at Google Research we have been using word n-gram models for a variety\n",
    "of R&D projects, such as statistical machine translation, speech recognition,\n",
    "spelling correction, entity detection, information extraction, and others.\n",
    "That's why we decided to share this enormous dataset with everyone. We\n",
    "processed 1,024,908,267,229 words of running text and are publishing the\n",
    "counts for all 1,176,470,663 five-word sequences that appear at least 40\n",
    "times. There are 13,588,391 unique words, after discarding words that appear\n",
    "less than 200 times.”\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-13T16:11:39.171440Z",
     "iopub.status.busy": "2020-08-13T16:11:39.170893Z",
     "iopub.status.idle": "2020-08-13T16:11:39.173847Z",
     "shell.execute_reply": "2020-08-13T16:11:39.173497Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/jupyterdays/lib/python3.7/site-packages/IPython/core/display.py:717: UserWarning: Consider using IPython.display.IFrame instead\n",
      "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe src=https://books.google.com/ngrams/ width=1000 height=800></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://books.google.com/ngrams/\"\n",
    "HTML(\"<iframe src=%s width=1000 height=800></iframe>\"%url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-13T16:11:39.177617Z",
     "iopub.status.busy": "2020-08-13T16:11:39.177202Z",
     "iopub.status.idle": "2020-08-13T16:11:39.178925Z",
     "shell.execute_reply": "2020-08-13T16:11:39.179346Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_ngrams(ngrams, start_year, end_year, corpus, smoothing):\n",
    "    '''\n",
    "    Plots ngrams using the Googlr n-gram viewer. \n",
    "    \n",
    "    Parameters:\n",
    "    ---------------------\n",
    "    \n",
    "    ngrams: \n",
    "        String with the n-gram to be searched. Words must be separated by spaces.\n",
    "    start_year: \n",
    "        \n",
    "    end_year: to year\n",
    "    corpus: corpus to be used (21:Spanish, 15:English, etc. check https://books.google.com/ngrams for more)\n",
    "    smoothing: number of years to average \n",
    "    '''\n",
    "    ngrams = \"+\".join(ngrams.split())\n",
    "    url = (\"https://books.google.com/ngrams/graph?content=%s&year_start=%d&year_end=%d&corpus=%d&smoothing=%d\"%\n",
    "          (ngrams, start_year, end_year, corpus, smoothing))\n",
    "    return HTML(\"<iframe src=%s width=1000 height=650></iframe>\"%url)\n",
    "\n",
    "#plot_ngrams(\"data science, databases\", 1970, 2019, 21, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More advanced language models \n",
    "\n",
    "- Allow to use more history compared to bigram/trigram/4-gram models \n",
    "- Built using deep neural networks using the following techniques \n",
    "    - Long-short term memory models \n",
    "    - Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [GPT-3 language model](https://github.com/openai/gpt-3)\n",
    "\n",
    "- A state-of-the-art language model with 175 billion parameters!!\n",
    "- Task agnostic and works for many different tasks.   \n",
    "- Currently very popular (and overhyped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Limitations of language models \n",
    "\n",
    "- All these complicated models are based on the simple idea of predicting next word given a sequence of words. \n",
    "- These models do not have common-sense knowledge or the ability to reason or reflect. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Example where the model does reasonably well\n",
    "\n",
    "Q: How many eyes does a giraffe have?<br>\n",
    "A: A giraffe has two eyes.\n",
    "\n",
    "Q: How many legs does a frog have?<br>\n",
    "A: A frog has four legs.\n",
    "\n",
    "Q: Are there any animals with three legs?<br>\n",
    "A: No, there are no animals with three legs.\n",
    "\n",
    "Q: Why don't animals have three legs?<br>\n",
    "A: Animals don't have three legs because they would fall over.\n",
    "\n",
    "[Source](https://lacker.io/ai/2020/07/06/giving-gpt-3-a-turing-test.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example where the model demonstrates lack of common sense knowledge \n",
    "\n",
    "Q: Which is heavier, a mouse or an elephant?<br> \n",
    "A: An elephant is heavier than a mouse.\n",
    "\n",
    "Q: Which is heavier, a mouse or a toaster?<br>\n",
    "A: A toaster is heavier than a mouse.\n",
    "\n",
    "Q: Which is heavier, a toaster or a pencil?<br>\n",
    "A: A pencil is heavier than a toaster.\n",
    "\n",
    "[Source](https://lacker.io/ai/2020/07/06/giving-gpt-3-a-turing-test.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Beyond language generation \n",
    "\n",
    "- The same idea can be used to generate sequences beyond language. \n",
    "- Here is an example of generating music: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-13T16:11:39.182482Z",
     "iopub.status.busy": "2020-08-13T16:11:39.182056Z",
     "iopub.status.idle": "2020-08-13T16:11:39.184448Z",
     "shell.execute_reply": "2020-08-13T16:11:39.184033Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=https://magenta.tensorflow.org/performance-rnn width=1000 height=650></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://magenta.tensorflow.org/performance-rnn\"\n",
    "HTML(\"<iframe src=%s width=1000 height=650></iframe>\"%url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resources \n",
    "\n",
    "- See [this famous blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). \n",
    "- [How Do You Know a Human Wrote This?](https://www.nytimes.com/2020/07/29/opinion/gpt-3-ai-automation.html)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}